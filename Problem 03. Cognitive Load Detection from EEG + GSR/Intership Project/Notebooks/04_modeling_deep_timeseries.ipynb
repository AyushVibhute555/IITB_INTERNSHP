{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe08539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 1364 task windows.\n",
      "Pre-processing and cleaning all data windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1091/1091 [00:02<00:00, 537.65it/s]\n",
      "100%|██████████| 273/273 [00:00<00:00, 428.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete.\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 35/35 [00:20<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 1.1027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 35/35 [00:20<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 1.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 35/35 [00:21<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 1.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 35/35 [00:21<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 1.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 1.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 35/35 [00:43<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 1.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 35/35 [00:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 1.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 35/35 [00:47<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 1.0285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 1.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 1.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training Loss: 0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training Loss: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training Loss: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training Loss: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 35/35 [00:19<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training Loss: 0.8883\n",
      "--------------------------------------------------\n",
      "\n",
      "Deep Learning Model Accuracy: 0.4066\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.40      0.58      0.47        83\n",
      "      Medium       0.31      0.14      0.19        86\n",
      "        High       0.45      0.49      0.47       104\n",
      "\n",
      "    accuracy                           0.41       273\n",
      "   macro avg       0.39      0.40      0.38       273\n",
      "weighted avg       0.39      0.41      0.38       273\n",
      "\n",
      "\n",
      "Trained model saved successfully to: ../models/cnn_lstm_model.pt\n"
     ]
    }
   ],
   "source": [
    "# ## 04 - Modeling with Deep Learning for Time Series\n",
    "#\n",
    "# **Objective:** Train a deep learning model directly on the raw, windowed time-series data.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "PROCESSED_DATA_DIR = '../data/processed/'\n",
    "MODELS_DIR = '../models/'\n",
    "INPUT_FILE = os.path.join(PROCESSED_DATA_DIR, 'task_windows.pkl')\n",
    "MODEL_OUTPUT_FILE = os.path.join(MODELS_DIR, 'cnn_lstm_model.pt')\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "SEQUENCE_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "try:\n",
    "    task_windows = pd.read_pickle(INPUT_FILE)\n",
    "    print(f\"Loaded {len(task_windows)} task windows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {INPUT_FILE} not found. Run notebook 01 first.\")\n",
    "    exit()\n",
    "\n",
    "labels = [d['CognitiveLoad'] for d in task_windows]\n",
    "data = [(d['EEG_Data'], d['GSR_Data']) for d in task_windows]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# --- NEW, ROBUST APPROACH: Pre-process and Clean Data Windows ---\n",
    "print(\"Pre-processing and cleaning all data windows...\")\n",
    "\n",
    "def clean_and_scale_data(data_list):\n",
    "    cleaned_data = []\n",
    "    for eeg, gsr in tqdm(data_list):\n",
    "        # Scale the signals\n",
    "        scaled_eeg = StandardScaler().fit_transform(eeg)\n",
    "        scaled_gsr = StandardScaler().fit_transform(gsr.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Clean any potential NaN/infinity values resulting from zero-variance slices\n",
    "        cleaned_eeg = np.nan_to_num(scaled_eeg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        cleaned_gsr = np.nan_to_num(scaled_gsr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        cleaned_data.append((cleaned_eeg, cleaned_gsr))\n",
    "    return cleaned_data\n",
    "\n",
    "cleaned_train_data = clean_and_scale_data(train_data)\n",
    "cleaned_test_data = clean_and_scale_data(test_data)\n",
    "\n",
    "print(\"Data cleaning complete.\")\n",
    "\n",
    "# --- PyTorch Dataset Class (Now Simplified) ---\n",
    "class CognitiveLoadDataset(Dataset):\n",
    "    def __init__(self, data, labels, seq_len):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Data is already scaled and cleaned, we just pad/truncate\n",
    "        eeg, gsr = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if eeg.shape[0] < self.seq_len:\n",
    "            pad_width = self.seq_len - eeg.shape[0]\n",
    "            eeg = np.pad(eeg, ((0, pad_width), (0, 0)), 'constant')\n",
    "        else:\n",
    "            eeg = eeg[:self.seq_len, :]\n",
    "\n",
    "        if gsr.shape[0] < self.seq_len:\n",
    "            pad_width = self.seq_len - gsr.shape[0]\n",
    "            gsr = np.pad(gsr, (0, pad_width), 'constant')\n",
    "        else:\n",
    "            gsr = gsr[:self.seq_len]\n",
    "\n",
    "        eeg_tensor = torch.FloatTensor(eeg).permute(1, 0)\n",
    "        gsr_tensor = torch.FloatTensor(gsr).unsqueeze(0)\n",
    "        return eeg_tensor, gsr_tensor, torch.LongTensor([label]).squeeze()\n",
    "\n",
    "# Use the pre-cleaned data to create the datasets\n",
    "train_dataset = CognitiveLoadDataset(cleaned_train_data, train_labels, SEQUENCE_LENGTH)\n",
    "test_dataset = CognitiveLoadDataset(cleaned_test_data, test_labels, SEQUENCE_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Define the CNN-LSTM Model ---\n",
    "class CN_LSTM_Model(nn.Module):\n",
    "    def __init__(self, eeg_channels=20, gsr_channels=1, num_classes=3):\n",
    "        super(CN_LSTM_Model, self).__init__()\n",
    "        self.eeg_cnn = nn.Sequential(nn.Conv1d(eeg_channels, 32, 3, padding=1), nn.ReLU(), nn.Conv1d(32, 64, 3, padding=1), nn.ReLU())\n",
    "        self.eeg_lstm = nn.LSTM(64, 50, 1, batch_first=True)\n",
    "        self.gsr_cnn = nn.Sequential(nn.Conv1d(gsr_channels, 16, 3, padding=1), nn.ReLU(), nn.Conv1d(16, 32, 3, padding=1), nn.ReLU())\n",
    "        self.gsr_lstm = nn.LSTM(32, 20, 1, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(50 + 20, 32), nn.ReLU(), nn.Dropout(0.5), nn.Linear(32, num_classes))\n",
    "\n",
    "    def forward(self, eeg, gsr):\n",
    "        eeg_out = self.eeg_cnn(eeg).permute(0, 2, 1)\n",
    "        eeg_out, _ = self.eeg_lstm(eeg_out)\n",
    "        gsr_out = self.gsr_cnn(gsr).permute(0, 2, 1)\n",
    "        gsr_out, _ = self.gsr_lstm(gsr_out)\n",
    "        combined = torch.cat((eeg_out[:, -1, :], gsr_out[:, -1, :]), dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "model = CN_LSTM_Model().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for eeg, gsr, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        eeg, gsr, labels = eeg.to(DEVICE), gsr.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(eeg, gsr)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for eeg, gsr, labels in test_loader:\n",
    "        outputs = model(eeg.to(DEVICE), gsr.to(DEVICE))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=['Low', 'Medium', 'High'], zero_division=0)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nDeep Learning Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# --- Save the Model ---\n",
    "torch.save(model.state_dict(), MODEL_OUTPUT_FILE)\n",
    "print(f\"\\nTrained model saved successfully to: {MODEL_OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
