{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1760729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for 38 participants...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Participants: 100%|██████████| 38/38 [01:30<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete.\n",
      "Total number of VALID task windows created: 1364\n",
      "Processed data saved to: ../data/processed/task_windows.pkl\n"
     ]
    }
   ],
   "source": [
    "# ## 01 - Data Preprocessing\n",
    "#\n",
    "# *Objective:* Load the raw EEG, GSR, and PSY files for all 38 participants,\n",
    "# clean the data, synchronize timestamps, and create structured \"data windows\" for each task.\n",
    "#\n",
    "# *Output:* A single pickle file (task_windows.pkl) containing a list of all valid data windows.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_DIR = '../data/raw/' # Corrected path to be case-sensitive\n",
    "PROCESSED_DATA_DIR = '../data/processed/' # Corrected path\n",
    "NUM_PARTICIPANTS = 38\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DATA_DIR, 'task_windows.pkl')\n",
    "\n",
    "# Create the processed data directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "all_participants_windows = []\n",
    "\n",
    "print(f\"Starting preprocessing for {NUM_PARTICIPANTS} participants...\")\n",
    "\n",
    "for participant_id in tqdm(range(1, NUM_PARTICIPANTS + 1), desc=\"Processing Participants\"):\n",
    "    try:\n",
    "        # Construct file paths\n",
    "        eeg_file = os.path.join(RAW_DATA_DIR, f'{participant_id}_EEG.csv')\n",
    "        gsr_file = os.path.join(RAW_DATA_DIR, f'{participant_id}_GSR.csv')\n",
    "        psy_file = os.path.join(RAW_DATA_DIR, f'{participant_id}_PSY.csv')\n",
    "\n",
    "        # Load data\n",
    "        eeg_df = pd.read_csv(eeg_file, low_memory=False)\n",
    "        gsr_df = pd.read_csv(gsr_file, low_memory=False)\n",
    "        psy_df = pd.read_csv(psy_file, low_memory=False)\n",
    "\n",
    "        # --- Data Cleaning and Timestamp Conversion ---\n",
    "        # Aggressively clean non-numeric and infinite values from the start\n",
    "        eeg_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        gsr_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        eeg_df['UnixTime'] = pd.to_numeric(eeg_df['UnixTime'], errors='coerce')\n",
    "        gsr_df['UnixTime'] = pd.to_numeric(gsr_df['UnixTime'], errors='coerce')\n",
    "        psy_df['routineStart'] = pd.to_numeric(psy_df['routineStart'], errors='coerce')\n",
    "        psy_df['routineEnd'] = pd.to_numeric(psy_df['routineEnd'], errors='coerce')\n",
    "\n",
    "        eeg_df.dropna(subset=['UnixTime'], inplace=True)\n",
    "        gsr_df.dropna(subset=['UnixTime'], inplace=True)\n",
    "        psy_df.dropna(subset=['routineStart', 'routineEnd'], inplace=True)\n",
    "        \n",
    "        # Define cognitive load labels\n",
    "        label_mapping = {1: 0, 2: 1, 3: 2} # Low, Medium, High\n",
    "        psy_df['CognitiveLoad'] = psy_df['Category'].map(label_mapping)\n",
    "        \n",
    "        # Define the exact columns we need\n",
    "        eeg_cols = ['Delta_TP9', 'Theta_TP9', 'Alpha_TP9', 'Beta_TP9', 'Gamma_TP9',\n",
    "                    'Delta_AF7', 'Theta_AF7', 'Alpha_AF7', 'Beta_AF7', 'Gamma_AF7',\n",
    "                    'Delta_AF8', 'Theta_AF8', 'Alpha_AF8', 'Beta_AF8', 'Gamma_AF8',\n",
    "                    'Delta_TP10', 'Theta_TP10', 'Alpha_TP10', 'Beta_TP10', 'Gamma_TP10']\n",
    "        gsr_col = 'GSR Conductance CAL'\n",
    "\n",
    "        # --- Windowing ---\n",
    "        for _, task in psy_df.iterrows():\n",
    "            start_time = task['routineStart']\n",
    "            end_time = task['routineEnd']\n",
    "            \n",
    "            eeg_slice = eeg_df[(eeg_df['UnixTime'] >= start_time) & (eeg_df['UnixTime'] <= end_time)]\n",
    "            gsr_slice = gsr_df[(gsr_df['UnixTime'] >= start_time) & (gsr_df['UnixTime'] <= end_time)]\n",
    "\n",
    "            # *** THE NEW, MORE ROBUST FIX IS HERE ***\n",
    "            # Instead of discarding windows with missing data, we repair them.\n",
    "            if not eeg_slice.empty and not gsr_slice.empty:\n",
    "                # Create copies to work on\n",
    "                eeg_slice_clean = eeg_slice.copy()\n",
    "                gsr_slice_clean = gsr_slice.copy()\n",
    "\n",
    "                # Use linear interpolation to fill gaps, then fill any remaining NaNs with 0\n",
    "                eeg_slice_clean[eeg_cols] = eeg_slice_clean[eeg_cols].interpolate(method='linear', limit_direction='both').fillna(0)\n",
    "                gsr_slice_clean[gsr_col] = gsr_slice_clean[gsr_col].interpolate(method='linear', limit_direction='both').fillna(0)\n",
    "                \n",
    "                all_participants_windows.append({\n",
    "                    'Participant': participant_id,\n",
    "                    'TaskKey': task['Key'],\n",
    "                    'CognitiveLoad': task['CognitiveLoad'],\n",
    "                    'EEG_Data': eeg_slice_clean[eeg_cols].values,\n",
    "                    'GSR_Data': gsr_slice_clean[gsr_col].values\n",
    "                })\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Files for participant {participant_id} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# --- Save the final processed data ---\n",
    "pd.to_pickle(all_participants_windows, OUTPUT_FILE)\n",
    "\n",
    "print(f\"\\nPreprocessing complete.\")\n",
    "print(f\"Total number of VALID task windows created: {len(all_participants_windows)}\")\n",
    "print(f\"Processed data saved to: {OUTPUT_FILE}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
